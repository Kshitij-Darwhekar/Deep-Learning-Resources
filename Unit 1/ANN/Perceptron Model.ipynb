{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Perceptron Model.ipynb","provenance":[],"mount_file_id":"1nWlIRKX2zrWxUdOj-5XrA5eu9IaqH6L7","authorship_tag":"ABX9TyNYvu3hoyv8tR+cSceKNrH/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"id":"-iC8MAnTgElZ","executionInfo":{"status":"ok","timestamp":1656995753448,"user_tz":-330,"elapsed":531,"user":{"displayName":"Smita Kulkarni","userId":"08881354177496898482"}}},"outputs":[],"source":["from sklearn.linear_model import Perceptron\n","#https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html\n","\n","# define model\n","model = Perceptron(eta0=1.0,max_iter=1000)"]},{"cell_type":"code","source":["# test classification dataset\n","from sklearn.datasets import make_classification\n","# define dataset\n","X, y = make_classification(n_samples=1000, n_features=10, n_informative=10, n_redundant=0, random_state=1)\n","# summarize the dataset\n","print(X.shape, y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JywvkW_jpAO5","executionInfo":{"status":"ok","timestamp":1656995823199,"user_tz":-330,"elapsed":726,"user":{"displayName":"Smita Kulkarni","userId":"08881354177496898482"}},"outputId":"0bc16700-cd31-4908-9095-ee1a3591fbf6"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["(1000, 10) (1000,)\n"]}]},{"cell_type":"code","source":["#We can fit and evaluate a Perceptron model using repeated stratified k-fold cross-validation via the RepeatedStratifiedKFold class. \n","#We will use 10 folds and three repeats in the test harness.\n","# evaluate a perceptron model on the dataset\n","from numpy import mean\n","from numpy import std\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.linear_model import Perceptron\n","# define dataset\n","X, y = make_classification(n_samples=1000, n_features=10, n_informative=10, n_redundant=0, random_state=1)\n","# define model\n","model = Perceptron()\n","# define model evaluation method\n","cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n","# evaluate model\n","scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n","# summarize result\n","print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vx6d0JWxpbZx","executionInfo":{"status":"ok","timestamp":1656995930803,"user_tz":-330,"elapsed":1600,"user":{"displayName":"Smita Kulkarni","userId":"08881354177496898482"}},"outputId":"f4e3ee0a-1cdd-4c24-ee7d-b46d259e937e"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Accuracy: 0.847 (0.052)\n"]}]},{"cell_type":"markdown","source":["Running the example evaluates the Perceptron algorithm on the synthetic dataset and reports the average accuracy across the three repeats of 10-fold cross-validation.\n","\n","Your specific results may vary given the stochastic nature of the learning algorithm. Consider running the example a few times.\n","\n","In this case, we can see that the model achieved a mean accuracy of about 84.7 percent."],"metadata":{"id":"FaNvsb7JrQlk"}},{"cell_type":"markdown","source":["We may decide to use the Perceptron classifier as our final model and make predictions on new data.\n","\n","This can be achieved by fitting the model pipeline on all available data and calling the predict() function passing in a new row of data."],"metadata":{"id":"8t5Nmw75sCEl"}},{"cell_type":"code","source":["from sklearn.datasets import make_classification\n","from sklearn.linear_model import Perceptron\n","# define dataset\n","X, y = make_classification(n_samples=1000, n_features=10, n_informative=10, n_redundant=0, random_state=1)\n","# define model\n","model = Perceptron()\n","# fit model\n","model.fit(X, y)\n","# define new data\n","row = [0.12777556,-3.64400522,-2.23268854,-1.82114386,1.75466361,0.1243966,1.03397657,2.35822076,1.01001752,0.56768485]\n","# make a prediction\n","yhat = model.predict([row])\n","# summarize prediction\n","print('Predicted Class: %d' % yhat)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Mr4EJPhriRo","executionInfo":{"status":"ok","timestamp":1656996411980,"user_tz":-330,"elapsed":615,"user":{"displayName":"Smita Kulkarni","userId":"08881354177496898482"}},"outputId":"058b778c-60b2-4ae4-f6e7-18ddb4d140e7"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted Class: 1\n"]}]},{"cell_type":"markdown","source":["### **Tune Perceptron Hyperparameters**\n","\n","The hyperparameters for the Perceptron algorithm must be configured for your specific dataset.\n","\n","**Perhaps the most important hyperparameter is the learning rate.**\n","\n","A large learning rate can cause the model to learn fast, but perhaps at the cost of lower skill. A smaller learning rate can result in a better-performing model but may take a long time to train the model.\n","\n","\n","It is common to test learning rates on a log scale between a small value such as 1e-4 (or smaller) and 1.0. We will test the following values in this case:"],"metadata":{"id":"q1FiP9zcuHDW"}},{"cell_type":"code","source":["from sklearn.datasets import make_classification\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.linear_model import Perceptron\n","# define dataset\n","X, y = make_classification(n_samples=1000, n_features=10, n_informative=10, n_redundant=0, random_state=1)\n","# define model\n","model = Perceptron()\n","# define model evaluation method\n","cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n","# define grid\n","grid = dict()\n","grid['eta0'] = [0.0001, 0.001, 0.01, 0.1, 1.0]\n","# define search\n","search = GridSearchCV(model, grid, scoring='accuracy', cv=cv, n_jobs=-1)\n","# perform the search\n","results = search.fit(X, y)\n","# summarize\n","print('Mean Accuracy: %.3f' % results.best_score_)\n","print('Config: %s' % results.best_params_)\n","# summarize all\n","means = results.cv_results_['mean_test_score']\n","params = results.cv_results_['params']\n","for mean, param in zip(means, params):\n","    print(\">%.3f with: %r\" % (mean, param))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K_FCgucOr0oB","executionInfo":{"status":"ok","timestamp":1656997084839,"user_tz":-330,"elapsed":1525,"user":{"displayName":"Smita Kulkarni","userId":"08881354177496898482"}},"outputId":"3e632cb6-cc18-45a8-a7e3-22882c32f449"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Accuracy: 0.857\n","Config: {'eta0': 0.0001}\n",">0.857 with: {'eta0': 0.0001}\n",">0.857 with: {'eta0': 0.001}\n",">0.853 with: {'eta0': 0.01}\n",">0.847 with: {'eta0': 0.1}\n",">0.847 with: {'eta0': 1.0}\n"]}]},{"cell_type":"markdown","source":["In this case, we can see that a smaller learning rate than the default results in better performance with learning rate 0.0001 and 0.001 both achieving a classification accuracy of about 85.7 percent as compared to the default of 1.0 that achieved an accuracy of about 84.7 percent."],"metadata":{"id":"Itsc1bDUwbjs"}},{"cell_type":"markdown","source":["**Another important hyperparameter is how many epochs are used to train the model.**\n","\n","This may depend on the training dataset and could vary greatly. Again, we will explore configuration values on a log scale between 1 and 1e+4."],"metadata":{"id":"GOKI59bDwyx8"}},{"cell_type":"markdown","source":["Running the example will evaluate each combination of configurations using repeated cross-validation.\n","\n","Your specific results may vary given the stochastic nature of the learning algorithm. Try running the example a few times.\n","\n","In this case, we can see that epochs 10 to 10,000 result in about the same classification accuracy. An interesting exception would be to explore configuring learning rate and number of training epochs at the same time to see if better results can be achieved."],"metadata":{"id":"jkwr9HO9xMwE"}},{"cell_type":"code","source":["# grid search total epochs for the perceptron\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.linear_model import Perceptron\n","# define dataset\n","X, y = make_classification(n_samples=1000, n_features=10, n_informative=10, n_redundant=0, random_state=1)\n","# define model\n","model = Perceptron(eta0=0.0001)\n","# define model evaluation method\n","cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n","# define grid\n","grid = dict()\n","grid['max_iter'] = [1, 10, 100, 1000, 10000]\n","# define search\n","search = GridSearchCV(model, grid, scoring='accuracy', cv=cv, n_jobs=-1)\n","# perform the search\n","results = search.fit(X, y)\n","# summarize\n","print('Mean Accuracy: %.3f' % results.best_score_)\n","print('Config: %s' % results.best_params_)\n","# summarize all\n","means = results.cv_results_['mean_test_score']\n","params = results.cv_results_['params']\n","for mean, param in zip(means, params):\n","    print(\">%.3f with: %r\" % (mean, param))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b-u30LqIxPP_","executionInfo":{"status":"ok","timestamp":1656997916704,"user_tz":-330,"elapsed":643,"user":{"displayName":"Smita Kulkarni","userId":"08881354177496898482"}},"outputId":"5146dea8-3f41-48d4-b2dd-35ddde155d5c"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Accuracy: 0.857\n","Config: {'max_iter': 10}\n",">0.850 with: {'max_iter': 1}\n",">0.857 with: {'max_iter': 10}\n",">0.857 with: {'max_iter': 100}\n",">0.857 with: {'max_iter': 1000}\n",">0.857 with: {'max_iter': 10000}\n"]}]},{"cell_type":"markdown","source":["## **1. Making Predictions**\n","\n","The first step is to develop a function that can make predictions.\n","\n","This will be needed both in the evaluation of candidate weights values in stochastic gradient descent, and after the model is finalized and we wish to start making predictions on test data or new data.\n","\n","Below is a function named predict() that predicts an output value for a row given a set of weights.\n","\n","The first weight is always the bias as it is standalone and not responsible for a specific input value."],"metadata":{"id":"cs5kxU_L0g0R"}},{"cell_type":"code","source":["#https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)\n","\n","# Make a prediction with weights\n","def predict(row, weights):\n","\tactivation = weights[0]\n","\tfor i in range(len(row)-1):\n","\t\tactivation += weights[i + 1] * row[i]\n","\treturn 1.0 if activation >= 0.0 else 0.0\n"," \n","# test predictions\n","dataset = [[2.7810836,2.550537003,0],\n","\t[1.465489372,2.362125076,0],\n","\t[3.396561688,4.400293529,0],\n","\t[1.38807019,1.850220317,0],\n","\t[3.06407232,3.005305973,0],\n","\t[7.627531214,2.759262235,1],\n","\t[5.332441248,2.088626775,1],\n","\t[6.922596716,1.77106367,1],\n","\t[8.675418651,-0.242068655,1],\n","\t[7.673756466,3.508563011,1]]\n","weights = [-0.1, 0.20653640140000007, -0.23418117710000003]\n","for row in dataset:\n","\tprediction = predict(row, weights)\n","\tprint(\"Expected=%d, Predicted=%d\" % (row[-1], prediction))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nJ_rSMG1xX4C","executionInfo":{"status":"ok","timestamp":1656998618542,"user_tz":-330,"elapsed":454,"user":{"displayName":"Smita Kulkarni","userId":"08881354177496898482"}},"outputId":"f614ab5b-f93b-48ce-9adc-54519dff3e78"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Expected=0, Predicted=0\n","Expected=0, Predicted=0\n","Expected=0, Predicted=0\n","Expected=0, Predicted=0\n","Expected=0, Predicted=0\n","Expected=1, Predicted=1\n","Expected=1, Predicted=1\n","Expected=1, Predicted=1\n","Expected=1, Predicted=1\n","Expected=1, Predicted=1\n"]}]},{"cell_type":"markdown","source":["## **2. Training Network Weights**\n","\n","We can estimate the weight values for our training data using stochastic gradient descent.\n","\n","Stochastic gradient descent requires two parameters:\n","\n","    Learning Rate: Used to limit the amount each weight is corrected each time it is updated.\n","    Epochs: The number of times to run through the training data while updating the weight.\n","\n","These, along with the training data will be the arguments to the function.\n","\n","There are 3 loops we need to perform in the function:\n","\n","    Loop over each epoch.\n","    Loop over each row in the training data for an epoch.\n","    Loop over each weight and update it for a row in an epoch.\n","\n","As you can see, we update each weight for each row in the training data, each epoch.\n","\n","Weights are updated based on the error the model made. The error is calculated as the difference between the expected output value and the prediction made with the candidate weights."],"metadata":{"id":"FgeUmdVM0160"}},{"cell_type":"code","source":["# Estimate Perceptron weights using stochastic gradient descent\n","def train_weights(train, l_rate, n_epoch):\n","\tweights = [0.0 for i in range(len(train[0]))]\n","\tfor epoch in range(n_epoch):\n","\t\tsum_error = 0.0\n","\t\tfor row in train:\n","\t\t\tprediction = predict(row, weights)\n","\t\t\terror = row[-1] - prediction\n","\t\t\tsum_error += error**2\n","\t\t\tweights[0] = weights[0] + l_rate * error\n","\t\t\tfor i in range(len(row)-1):\n","\t\t\t\tweights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n","\t\tprint('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n","\treturn weights"],"metadata":{"id":"0CMGaEW-02y0","executionInfo":{"status":"ok","timestamp":1656998824149,"user_tz":-330,"elapsed":1166,"user":{"displayName":"Smita Kulkarni","userId":"08881354177496898482"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"8kPH_wbO03yU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[""],"metadata":{"id":"q7uMY0F60_RE"}},{"cell_type":"code","source":["# Make a prediction with weights\n","def predict(row, weights):\n","\tactivation = weights[0]\n","\tfor i in range(len(row)-1):\n","\t\tactivation += weights[i + 1] * row[i]\n","\treturn 1.0 if activation >= 0.0 else 0.0\n"," \n","# Estimate Perceptron weights using stochastic gradient descent\n","def train_weights(train, l_rate, n_epoch):\n","\tweights = [0.0 for i in range(len(train[0]))]\n","\tfor epoch in range(n_epoch):\n","\t\tsum_error = 0.0\n","\t\tfor row in train:\n","\t\t\tprediction = predict(row, weights)\n","\t\t\terror = row[-1] - prediction\n","\t\t\tsum_error += error**2\n","\t\t\tweights[0] = weights[0] + l_rate * error\n","\t\t\tfor i in range(len(row)-1):\n","\t\t\t\tweights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n","\t\tprint('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n","\treturn weights\n"," \n","# Calculate weights\n","dataset = [[2.7810836,2.550537003,0],\n","\t[1.465489372,2.362125076,0],\n","\t[3.396561688,4.400293529,0],\n","\t[1.38807019,1.850220317,0],\n","\t[3.06407232,3.005305973,0],\n","\t[7.627531214,2.759262235,1],\n","\t[5.332441248,2.088626775,1],\n","\t[6.922596716,1.77106367,1],\n","\t[8.675418651,-0.242068655,1],\n","\t[7.673756466,3.508563011,1]]\n","l_rate = 0.1\n","n_epoch = 5\n","weights = train_weights(dataset, l_rate, n_epoch)\n","print(weights)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K4vyg-CP1B6M","executionInfo":{"status":"ok","timestamp":1656998868702,"user_tz":-330,"elapsed":414,"user":{"displayName":"Smita Kulkarni","userId":"08881354177496898482"}},"outputId":"979ab92e-f5de-4130-e625-834d7a7c63b6"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":[">epoch=0, lrate=0.100, error=2.000\n",">epoch=1, lrate=0.100, error=1.000\n",">epoch=2, lrate=0.100, error=0.000\n",">epoch=3, lrate=0.100, error=0.000\n",">epoch=4, lrate=0.100, error=0.000\n","[-0.1, 0.20653640140000007, -0.23418117710000003]\n"]}]},{"cell_type":"markdown","source":["We use a learning rate of 0.1 and train the model for only 5 epochs, or 5 exposures of the weights to the entire training dataset.\n","\n","Running the example prints a message each epoch with the sum squared error for that epoch and the final set of weights."],"metadata":{"id":"I6FgWrYh1MNs"}},{"cell_type":"markdown","source":[""],"metadata":{"id":"Ml3wNHLI25FU"}},{"cell_type":"markdown","source":["## **3. Modeling the Sonar Dataset**\n","\n","In this section, we will train a Perceptron model using stochastic gradient descent on the Sonar dataset.\n","\n","The example assumes that a CSV copy of the dataset is in the current working directory with the file name sonar.all-data.csv.\n","\n","The dataset is first loaded, the string values converted to numeric and the output column is converted from strings to the integer values of 0 to 1. This is achieved with helper functions load_csv(), str_column_to_float() and str_column_to_int() to load and prepare the dataset.\n","\n","We will use k-fold cross validation to estimate the performance of the learned model on unseen data. This means that we will construct and evaluate k models and estimate the performance as the mean model error. Classification accuracy will be used to evaluate each model. These behaviors are provided in the cross_validation_split(), accuracy_metric() and evaluate_algorithm() helper functions.\n","\n","We will use the predict() and train_weights() functions created above to train the model and a new perceptron() function to tie them together."],"metadata":{"id":"IkjtDbh527hd"}},{"cell_type":"code","source":["# Perceptron Algorithm on the Sonar Dataset\n","from random import seed\n","from random import randrange\n","from csv import reader\n"," \n","# Load a CSV file\n","def load_csv(filename):\n","\tdataset = list()\n","\twith open(filename, 'r') as file:\n","\t\tcsv_reader = reader(file)\n","\t\tfor row in csv_reader:\n","\t\t\tif not row:\n","\t\t\t\tcontinue\n","\t\t\tdataset.append(row)\n","\treturn dataset\n"," \n","# Convert string column to float\n","def str_column_to_float(dataset, column):\n","\tfor row in dataset:\n","\t\trow[column] = float(row[column].strip())\n"," \n","# Convert string column to integer\n","def str_column_to_int(dataset, column):\n","\tclass_values = [row[column] for row in dataset]\n","\tunique = set(class_values)\n","\tlookup = dict()\n","\tfor i, value in enumerate(unique):\n","\t\tlookup[value] = i\n","\tfor row in dataset:\n","\t\trow[column] = lookup[row[column]]\n","\treturn lookup\n"," \n","# Split a dataset into k folds\n","def cross_validation_split(dataset, n_folds):\n","\tdataset_split = list()\n","\tdataset_copy = list(dataset)\n","\tfold_size = int(len(dataset) / n_folds)\n","\tfor i in range(n_folds):\n","\t\tfold = list()\n","\t\twhile len(fold) < fold_size:\n","\t\t\tindex = randrange(len(dataset_copy))\n","\t\t\tfold.append(dataset_copy.pop(index))\n","\t\tdataset_split.append(fold)\n","\treturn dataset_split\n"," \n","# Calculate accuracy percentage\n","def accuracy_metric(actual, predicted):\n","\tcorrect = 0\n","\tfor i in range(len(actual)):\n","\t\tif actual[i] == predicted[i]:\n","\t\t\tcorrect += 1\n","\treturn correct / float(len(actual)) * 100.0\n"," \n","# Evaluate an algorithm using a cross validation split\n","def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n","\tfolds = cross_validation_split(dataset, n_folds)\n","\tscores = list()\n","\tfor fold in folds:\n","\t\ttrain_set = list(folds)\n","\t\ttrain_set.remove(fold)\n","\t\ttrain_set = sum(train_set, [])\n","\t\ttest_set = list()\n","\t\tfor row in fold:\n","\t\t\trow_copy = list(row)\n","\t\t\ttest_set.append(row_copy)\n","\t\t\trow_copy[-1] = None\n","\t\tpredicted = algorithm(train_set, test_set, *args)\n","\t\tactual = [row[-1] for row in fold]\n","\t\taccuracy = accuracy_metric(actual, predicted)\n","\t\tscores.append(accuracy)\n","\treturn scores\n"," \n","# Make a prediction with weights\n","def predict(row, weights):\n","\tactivation = weights[0]\n","\tfor i in range(len(row)-1):\n","\t\tactivation += weights[i + 1] * row[i]\n","\treturn 1.0 if activation >= 0.0 else 0.0\n"," \n","# Estimate Perceptron weights using stochastic gradient descent\n","def train_weights(train, l_rate, n_epoch):\n","\tweights = [0.0 for i in range(len(train[0]))]\n","\tfor epoch in range(n_epoch):\n","\t\tfor row in train:\n","\t\t\tprediction = predict(row, weights)\n","\t\t\terror = row[-1] - prediction\n","\t\t\tweights[0] = weights[0] + l_rate * error\n","\t\t\tfor i in range(len(row)-1):\n","\t\t\t\tweights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n","\treturn weights\n"," \n","# Perceptron Algorithm With Stochastic Gradient Descent\n","def perceptron(train, test, l_rate, n_epoch):\n","\tpredictions = list()\n","\tweights = train_weights(train, l_rate, n_epoch)\n","\tfor row in test:\n","\t\tprediction = predict(row, weights)\n","\t\tpredictions.append(prediction)\n","\treturn(predictions)\n"," \n","# Test the Perceptron algorithm on the sonar dataset\n","seed(1)\n","# load and prepare data\n","filename = 'sonar.all-data.csv'\n","dataset = load_csv(filename)\n","for i in range(len(dataset[0])-1):\n","\tstr_column_to_float(dataset, i)\n","# convert string class to integers\n","str_column_to_int(dataset, len(dataset[0])-1)\n","# evaluate algorithm\n","n_folds = 3\n","l_rate = 0.01\n","n_epoch = 500\n","scores = evaluate_algorithm(dataset, perceptron, n_folds, l_rate, n_epoch)\n","print('Scores: %s' % scores)\n","print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u6mTvcrv1DAk","executionInfo":{"status":"ok","timestamp":1656999865543,"user_tz":-330,"elapsed":3832,"user":{"displayName":"Smita Kulkarni","userId":"08881354177496898482"}},"outputId":"fd423632-6b80-4888-cf09-603b069e9597"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Scores: [76.81159420289855, 69.56521739130434, 72.46376811594203]\n","Mean Accuracy: 72.947%\n"]}]},{"cell_type":"markdown","source":["A k value of 3 was used for cross-validation, giving each fold 208/3 = 69.3 or just under 70 records to be evaluated upon each iteration. A learning rate of 0.1 and 500 training epochs were chosen with a little experimentation.\n","\n","You can try your own configurations and see if you can beat my score.\n","\n","Running this example prints the scores for each of the 3 cross-validation folds then prints the mean classification accuracy.\n","\n","We can see that the accuracy is about 72%, higher than the baseline value of just over 50% if we only predicted the majority class using the Zero Rule Algorithm."],"metadata":{"id":"MfMJgnbl4_JP"}},{"cell_type":"code","source":[""],"metadata":{"id":"e3wfRZct5ATQ"},"execution_count":null,"outputs":[]}]}